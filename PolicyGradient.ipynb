{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy.random as rand\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def theta_policy(n_states, n_actions) : \n",
    "    theta = np.zeros((n_states, n_actions))    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\theta  - \\textrm{ parametrized policy}, \\theta \\in R^{ n \\times k }, \\textrm{where} $$\n",
    "$$ n=|S| - \\text{number of states} ,$$\n",
    "$$ k=|A| - \\text{number of actions} , $$\n",
    "$$ \\theta_{ij}   - \\textrm{ policy for state i  and action j }$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "theta = theta_policy(5, 2)\n",
    "print theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mu_policy( theta  ):\n",
    "    n_states, n_actions = theta.shape\n",
    "    mu  = np.zeros((n_states, n_actions))\n",
    "    for state in range(0, n_states) : \n",
    "        max_theta = np.max(theta[state])\n",
    "        mu[state] = np.exp(theta[state]-max_theta) / np.sum(np.exp(theta[state] - max_theta))\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mu - \\text{softmax policy} $$\n",
    "$$ $$ \n",
    "$$ \\mu(a=j | s=i; \\theta) = \\mu_{\\theta}(i,j) $$\n",
    "$$ $$\n",
    "$$  \\mu_{\\theta}(i,j)   =  \\frac{\\exp{\\theta_{ij}}}{\\sum_t{\\exp{\\theta_{it}}}} \\ $$\n",
    "\n",
    "$\\text{ computed using sum-exp trick} : $\n",
    "\n",
    "$$  \\frac{\\exp{\\theta_{ij}}}{\\sum_t{\\exp{\\theta_{it}}}} = \\frac{\\exp{\\theta_{ij}} \\times \\exp{(-a)}}{\\sum_t{\\exp{\\theta_{it}}} \\times \\exp{(-a)} } =  \\frac{\\exp{(\\theta_{ij} - a)}}{\\sum_t{\\exp{(\\theta_{it} - a)}}} $$\n",
    "\n",
    "$$ a = \\max_{t}{\\theta_{it}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5  0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]]\n"
     ]
    }
   ],
   "source": [
    "mu = mu_policy(theta)\n",
    "print mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_mu_policy( theta  ):\n",
    "    n_states, n_actions = theta.shape\n",
    "    log_mu  = np.zeros((n_states, n_actions))\n",
    "    for state in range(0, n_states) : \n",
    "        max_theta = np.max(theta[state])\n",
    "        log_sum_exp = max_theta  +  np.log(np.sum(np.exp(theta[state] - max_theta)))\n",
    "        log_mu[state] = theta[state] - log_sum_exp\n",
    "    return log_mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\log \\mu_{\\theta}(i,j)   = \\log \\frac{\\exp{\\theta_{ij}}}{\\sum_t{\\exp{\\theta_{it}}}} \\  = \\theta_{ij}  - \\log \\sum_t{\\exp{\\theta_{it}}}  = \\theta_{ij} - ( a + \\log \\sum_t{\\exp{(\\theta_{it}-a})}  )$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.69314718 -0.69314718]\n",
      " [-0.69314718 -0.69314718]\n",
      " [-0.69314718 -0.69314718]\n",
      " [-0.69314718 -0.69314718]\n",
      " [-0.69314718 -0.69314718]]\n"
     ]
    }
   ],
   "source": [
    "log_mu = log_mu_policy(theta)\n",
    "print log_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_log_mu_policy(theta, mu ,  state, action) :\n",
    "    n_states, n_actions = theta.shape\n",
    "    grad  = np.zeros((n_states, n_actions))\n",
    "    grad[state, action] = 1 \n",
    "    max_theta = np.max(theta[state])\n",
    "    grad[state] = grad[state] - mu[state]\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\nabla \\log \\mu_{\\theta}(i,j)  = \\nabla( \\theta_{ij}  - \\log \\sum_t{\\exp{\\theta_{it}}} )  = \\begin{pmatrix}\n",
    "  \\frac{\\partial \\log \\mu_{\\theta}(i,j)} {\\partial \\theta_{11}}  &  \\frac{\\partial \\log \\mu_{\\theta}(i,j)} {\\partial \\theta_{12}} & \\cdots &  \\frac{\\partial \\log \\mu_{\\theta}(i,j)} {\\partial \\theta_{1m}} \\\\\n",
    "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "  \\frac{\\partial \\log \\mu_{\\theta}(i,j)} {\\partial \\theta_{n1}} &  \\frac{\\partial \\log \\mu_{\\theta}(i,j)} {\\partial \\theta_{n2}} & \\cdots & \\frac{\\partial \\log \\mu_{\\theta}(i,j)} {\\partial \\theta_{nm}} \n",
    " \\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial ( \\log \\sum_t{\\exp{\\theta_{pt}}} ) } {\\partial \\theta_{pl}} =  \\frac{\\exp{\\theta_{pl}}}{\\sum_t{\\exp{\\theta_{pt}}} }  = \\mu_{\\theta}(p,l) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial \\log \\mu_{\\theta}(i,j)} {\\partial \\theta_{pl}} =  \\frac{\\partial ( \\theta_{ij}  - \\log \\sum_t{\\exp{\\theta_{it}}} ) } {\\partial \\theta_{pl}}  =  \\begin{cases}\n",
    "    1 -  \\mu_{\\theta}(p,l)      & \\quad \\text{if } i=p, j=l \\\\\n",
    "    -  \\mu_{\\theta}(p,l)     & \\quad \\text{if } i=p, j \\ne l \\\\\n",
    "    0                           & \\quad \\text{if }  i \\ne p, j \\ne l  \\\\\n",
    "  \\end{cases} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient log mu :\n",
      "[[ 0.   0. ]\n",
      " [-0.5  0.5]\n",
      " [ 0.   0. ]\n",
      " [ 0.   0. ]\n",
      " [ 0.   0. ]]\n",
      "mu policy after adding gradient :\n",
      "[[ 0.5         0.5       ]\n",
      " [ 0.26894142  0.73105858]\n",
      " [ 0.5         0.5       ]\n",
      " [ 0.5         0.5       ]\n",
      " [ 0.5         0.5       ]]\n"
     ]
    }
   ],
   "source": [
    "print \"gradient log mu :\\n\", gradient_log_mu_policy(theta, mu, 1, 1)\n",
    "print \"mu policy after adding gradient :\\n\" , mu_policy(theta + gradient_log_mu_policy(theta, mu, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def path_likelihood_ratio(theta_policy, mu_policy, path_states,  path_actions ) :\n",
    "    likelihood_ratio = np.zeros(theta_policy.shape)\n",
    "    path_len = len(path_states)\n",
    "    for t in range(0, path_len) :\n",
    "        likelihood_ratio = likelihood_ratio + gradient_log_mu_policy(theta_policy, mu_policy , path_states[t] , path_actions[t])\n",
    "    return likelihood_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likelihood ratio of path: \n",
    "$$ u(\\xi;\\theta)  =  \\sum_t{ \\nabla \\log \\mu_{\\theta}(s_t,a_t) } $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5  0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]]\n",
      "path state    :  [2 0 1 2 3]\n",
      "path  actions :  [1 0 0 0 1]\n",
      "path reward :  1.6561\n",
      "path likelihood ratio: \n",
      "[[ 0.5 -0.5]\n",
      " [ 0.5 -0.5]\n",
      " [ 0.   0. ]\n",
      " [-0.5  0.5]\n",
      " [ 0.   0. ]]\n",
      "path state    :  [3 0 1 0 0]\n",
      "path  actions :  [1 0 1 0 1]\n",
      "path reward :  1.81\n",
      "path likelihood ratio: \n",
      "[[ 0.5 -0.5]\n",
      " [-0.5  0.5]\n",
      " [ 0.   0. ]\n",
      " [-0.5  0.5]\n",
      " [ 0.   0. ]]\n",
      "path state    :  [2 3 4 4 4]\n",
      "path  actions :  [0 0 0 0 1]\n",
      "path reward :  7.4682\n",
      "path likelihood ratio: \n",
      "[[ 0.   0. ]\n",
      " [ 0.   0. ]\n",
      " [ 0.5 -0.5]\n",
      " [ 0.5 -0.5]\n",
      " [ 0.5 -0.5]]\n",
      "path state    :  [2 0 1 2 3]\n",
      "path  actions :  [0 0 0 0 0]\n",
      "path reward :  0.0\n",
      "path likelihood ratio: \n",
      "[[ 0.5 -0.5]\n",
      " [ 0.5 -0.5]\n",
      " [ 1.  -1. ]\n",
      " [ 0.5 -0.5]\n",
      " [ 0.   0. ]]\n",
      "path state    :  [0 0 1 2 0]\n",
      "path  actions :  [1 0 0 1 0]\n",
      "path reward :  0.729\n",
      "path likelihood ratio: \n",
      "[[ 0.5 -0.5]\n",
      " [ 0.5 -0.5]\n",
      " [-0.5  0.5]\n",
      " [ 0.   0. ]\n",
      " [ 0.   0. ]]\n",
      "path state    :  [2 0 1 0 1]\n",
      "path  actions :  [1 0 1 0 0]\n",
      "path reward :  1.81\n",
      "path likelihood ratio: \n",
      "[[ 1.  -1. ]\n",
      " [ 0.   0. ]\n",
      " [-0.5  0.5]\n",
      " [ 0.   0. ]\n",
      " [ 0.   0. ]]\n",
      "path state    :  [4 0 1 0 1]\n",
      "path  actions :  [1 0 1 0 1]\n",
      "path reward :  3.4661\n",
      "path likelihood ratio: \n",
      "[[ 1.  -1. ]\n",
      " [-1.   1. ]\n",
      " [ 0.   0. ]\n",
      " [ 0.   0. ]\n",
      " [-0.5  0.5]]\n",
      "path state    :  [3 0 0 1 0]\n",
      "path  actions :  [1 1 0 1 0]\n",
      "path reward :  1.729\n",
      "path likelihood ratio: \n",
      "[[ 0.5 -0.5]\n",
      " [-0.5  0.5]\n",
      " [ 0.   0. ]\n",
      " [-0.5  0.5]\n",
      " [ 0.   0. ]]\n",
      "path state    :  [1 2 3 0 1]\n",
      "path  actions :  [0 0 1 0 0]\n",
      "path reward :  0.81\n",
      "path likelihood ratio: \n",
      "[[ 0.5 -0.5]\n",
      " [ 1.  -1. ]\n",
      " [ 0.5 -0.5]\n",
      " [-0.5  0.5]\n",
      " [ 0.   0. ]]\n",
      "path state    :  [1 2 0 0 0]\n",
      "path  actions :  [0 1 1 1 1]\n",
      "path reward :  0.9\n",
      "path likelihood ratio: \n",
      "[[-1.5  1.5]\n",
      " [ 0.5 -0.5]\n",
      " [-0.5  0.5]\n",
      " [ 0.   0. ]\n",
      " [ 0.   0. ]]\n"
     ]
    }
   ],
   "source": [
    "print mu\n",
    "for i in range(0, paths_states.shape[0]) :\n",
    "    print \"path state    : \",paths_states[i]\n",
    "    print \"path  actions : \",paths_actions[i]\n",
    "    print \"path reward : \",paths_rewards[i]\n",
    "    print \"path likelihood ratio: \\n\", path_likelihood_ratio(theta, mu, paths_states[i], paths_actions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_gradient(theta_policy, mu_policy , paths_states , paths_actions, paths_rewards) :\n",
    "    policy_grad = np.zeros(theta_policy.shape)\n",
    "    n_paths , path_len =  paths_states.shape\n",
    "    for path in range(0, n_paths) :\n",
    "        policy_grad = policy_grad + paths_rewards[path]*path_likelihood_ratio(theta_policy, mu_policy, paths_states[path], paths_actions[path])\n",
    "    policy_grad = policy_grad/n_paths\n",
    "    return policy_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy gradient estimation :  \n",
    "$$ \\nabla \\eta(\\theta)  =  \\frac{1}{M} \\sum_m \\rho(\\xi_m) \\sum_t{ \\nabla \\log \\mu_{\\theta}(s_{mt},a_{mt}) } $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current policy mu : \n",
      "[[ 0.5  0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]]\n",
      "generated trajectories : \n",
      " states :\n",
      "[[1 0 0 1 0]\n",
      " [3 4 4 0 0]\n",
      " [3 0 0 1 2]\n",
      " [3 4 0 1 0]\n",
      " [1 2 0 0 1]\n",
      " [3 0 0 0 0]\n",
      " [4 0 1 0 1]\n",
      " [3 0 0 1 0]\n",
      " [2 3 4 4 4]\n",
      " [4 0 1 0 1]] \n",
      " actions :\n",
      "[[0 1 0 0 1]\n",
      " [0 0 1 1 0]\n",
      " [1 1 0 0 0]\n",
      " [0 1 0 1 1]\n",
      " [0 1 1 0 1]\n",
      " [1 1 1 1 0]\n",
      " [0 0 1 0 0]\n",
      " [1 1 0 1 1]\n",
      " [0 0 0 0 1]\n",
      " [1 0 1 0 1]] \n",
      " rewards :\n",
      "[ 0.      5.22    1.      2.529   1.5561  1.      4.81    1.729   7.4682\n",
      "  3.4661]\n",
      "policy gradient : \n",
      "[[ 0.64116  -0.64116 ]\n",
      " [-0.50951   0.50951 ]\n",
      " [ 0.345605 -0.345605]\n",
      " [ 0.57441  -0.57441 ]\n",
      " [ 0.314155 -0.314155]]\n",
      "new mu policy : \n",
      "[[ 0.78284443  0.21715557]\n",
      " [ 0.26521834  0.73478166]\n",
      " [ 0.66623604  0.33376396]\n",
      " [ 0.75929532  0.24070468]\n",
      " [ 0.65210616  0.34789384]]\n"
     ]
    }
   ],
   "source": [
    "print \"current policy mu : \\n\", mu_policy(theta)\n",
    "print \"generated trajectories : \\n states :\\n\" , paths_states, '\\n actions :\\n',  paths_actions, '\\n rewards :\\n', paths_rewards\n",
    "print \"policy gradient : \\n\",  policy_gradient(theta , mu, paths_states, paths_actions , paths_rewards)\n",
    "print \"new mu policy : \\n\",  mu_policy(theta + policy_gradient(theta , mu, paths_states, paths_actions , paths_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def policy_gradient_algo(transitions, rewards, discount, path_len=10,  n_paths=100, gamma=1.0, eps=0.01, n_iterations=100, logging=False) : \n",
    "    n_states, n_actions = rewards.shape\n",
    "    theta = theta_policy(n_states, n_actions)\n",
    "    mu = mu_policy(theta)\n",
    "    n=0\n",
    "    paths_states, paths_actions, paths_rewards = generate_rollouts(mu, transitions, rewards, discount , path_len, n_paths )\n",
    "    pgrad = policy_gradient(theta, mu,  paths_states, paths_actions, paths_rewards) \n",
    "    theta_diff =  (gamma/(n+1))*pgrad \n",
    "    theta_diff_norm = np.linalg.norm(theta_diff)\n",
    "    #mu_diff = np.linalg.norm(mu_policy(theta) - mu_policy(theta+pgrad))\n",
    "    while ( (n<n_iterations) & (theta_diff_norm>eps) ):\n",
    "        if (logging) :\n",
    "            print \"mu policy : \\n\",  mu_policy(theta)\n",
    "            print \"policy gradient: \\n\", pgrad\n",
    "            print \"theta policy : \\n\" , theta_diff\n",
    "            print \"theta policy diff norm: \" , theta_diff_norm\n",
    "        theta_diff =  (gamma/(n+1))*pgrad\n",
    "        theta = theta + theta_diff\n",
    "        mu = mu_policy(theta)\n",
    "        paths_states, paths_actions, paths_rewards = generate_rollouts(mu_policy(theta), transitions, rewards, discount , path_len, n_paths )\n",
    "        pgrad = policy_gradient(theta, mu , paths_states, paths_actions, paths_rewards)\n",
    "        #mu_diff = np.linalg.norm(mu_policy(theta) - mu_policy(theta+pgrad))\n",
    "        theta_diff_norm = np.linalg.norm(theta_diff) \n",
    "        n = n+1\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "repeat\n",
    "$$ \\theta^{n+1} = \\theta^{n} + \\frac {\\gamma}{n}  * \\nabla \\eta(\\theta^n) $$\n",
    "until \n",
    "$$ || \\theta^{n+1} - \\theta^{n} || > \\epsilon$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu policy : \n",
      "[[ 0.5  0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]]\n",
      "policy gradient: \n",
      "[[ 0.21692377 -0.21692377]\n",
      " [ 0.16301536 -0.16301536]\n",
      " [-0.05397061  0.05397061]\n",
      " [ 0.21603465 -0.21603465]\n",
      " [ 0.69205387 -0.69205387]]\n",
      "theta policy : \n",
      "[[ 2.16923771 -2.16923771]\n",
      " [ 1.63015355 -1.63015355]\n",
      " [-0.5397061   0.5397061 ]\n",
      " [ 2.16034651 -2.16034651]\n",
      " [ 6.92053867 -6.92053867]]\n",
      "theta policy diff norm:  10.974081095\n",
      "mu policy : \n",
      "[[  9.87111854e-01   1.28881457e-02]\n",
      " [  9.63041723e-01   3.69582773e-02]\n",
      " [  2.53617270e-01   7.46382730e-01]\n",
      " [  9.86883655e-01   1.31163447e-02]\n",
      " [  9.99999025e-01   9.74756340e-07]]\n",
      "policy gradient: \n",
      "[[ -4.18431385e-02   4.18431385e-02]\n",
      " [ -5.14488605e-02   5.14488605e-02]\n",
      " [  6.35427288e-01  -6.35427288e-01]\n",
      " [  5.21880098e-02  -5.21880098e-02]\n",
      " [  5.02841783e-05  -5.02841783e-05]]\n",
      "theta policy : \n",
      "[[ 2.16923771 -2.16923771]\n",
      " [ 1.63015355 -1.63015355]\n",
      " [-0.5397061   0.5397061 ]\n",
      " [ 2.16034651 -2.16034651]\n",
      " [ 6.92053867 -6.92053867]]\n",
      "theta policy diff norm:  10.974081095\n",
      "mu policy : \n",
      "[[  9.80545755e-01   1.94542447e-02]\n",
      " [  9.39676759e-01   6.03232406e-02]\n",
      " [  9.94907398e-01   5.09260240e-03]\n",
      " [  9.92174979e-01   7.82502062e-03]\n",
      " [  9.99999026e-01   9.74266315e-07]]\n",
      "policy gradient: \n",
      "[[  1.89249202e-01  -1.89249202e-01]\n",
      " [  1.81025194e-01  -1.81025194e-01]\n",
      " [  5.52018765e-02  -5.52018765e-02]\n",
      " [  1.00048553e-01  -1.00048553e-01]\n",
      " [  8.46078711e-05  -8.46078711e-05]]\n",
      "theta policy : \n",
      "[[ -2.09215693e-01   2.09215693e-01]\n",
      " [ -2.57244303e-01   2.57244303e-01]\n",
      " [  3.17713644e+00  -3.17713644e+00]\n",
      " [  2.60940049e-01  -2.60940049e-01]\n",
      " [  2.51420892e-04  -2.51420892e-04]]\n",
      "theta policy diff norm:  4.53260004255\n",
      "mu policy : \n",
      "[[  9.94412974e-01   5.58702648e-03]\n",
      " [  9.81158149e-01   1.88418514e-02]\n",
      " [  9.96469831e-01   3.53016872e-03]\n",
      " [  9.95968445e-01   4.03155481e-03]\n",
      " [  9.99999026e-01   9.73716934e-07]]\n",
      "policy gradient: \n",
      "[[  6.12475192e-02  -6.12475192e-02]\n",
      " [  2.03210985e-01  -2.03210985e-01]\n",
      " [  3.65145527e-02  -3.65145527e-02]\n",
      " [  5.23770834e-02  -5.23770834e-02]\n",
      " [  9.66835924e-05  -9.66835923e-05]]\n",
      "theta policy : \n",
      "[[  6.30830674e-01  -6.30830674e-01]\n",
      " [  6.03417314e-01  -6.03417314e-01]\n",
      " [  1.84006255e-01  -1.84006255e-01]\n",
      " [  3.33495177e-01  -3.33495177e-01]\n",
      " [  2.82026237e-04  -2.82026237e-04]]\n",
      "theta policy diff norm:  1.34695004257\n",
      "mu policy : \n",
      "[[  9.95880695e-01   4.11930514e-03]\n",
      " [  9.93095874e-01   6.90412570e-03]\n",
      " [  9.97057198e-01   2.94280197e-03]\n",
      " [  9.96894434e-01   3.10556604e-03]\n",
      " [  9.99999027e-01   9.73246336e-07]]\n",
      "policy gradient: \n",
      "[[  3.23397442e-02  -3.23397442e-02]\n",
      " [  6.30282589e-02  -6.30282589e-02]\n",
      " [  3.08084227e-02  -3.08084227e-02]\n",
      " [  1.43675989e-02  -1.43675989e-02]\n",
      " [  9.67561166e-05  -9.67561166e-05]]\n",
      "theta policy : \n",
      "[[  1.53118798e-01  -1.53118798e-01]\n",
      " [  5.08027463e-01  -5.08027463e-01]\n",
      " [  9.12863817e-02  -9.12863817e-02]\n",
      " [  1.30942708e-01  -1.30942708e-01]\n",
      " [  2.41708981e-04  -2.41708981e-04]]\n",
      "theta policy diff norm:  0.783602608623\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.80865097, -2.80865097],\n",
       "       [ 2.61041054, -2.61041054],\n",
       "       [ 2.97433983, -2.97433983],\n",
       "       [ 2.91445964, -2.91445964],\n",
       "       [ 6.92150733, -6.92150733]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_gradient_algo(P, R , discount,  path_len=10, gamma = 10.0, n_iterations=5, logging = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare with other existing method  - policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mdptoolbox_copy.example as mdp_ex\n",
    "\n",
    "n_states = 5\n",
    "n_actions = 2\n",
    "fire_prob = 0.1\n",
    "discount=0.9\n",
    "n_paths=100\n",
    "path_len=100\n",
    "path_len = 10\n",
    "path_num = 10\n",
    "\n",
    "P, R = mdp_ex.forest(S=n_states, p=fire_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy (policy iteration) : \n",
      "(0, 0, 0, 0, 0)\n",
      "Optimal policy (policy gradient) :\n",
      "[[  9.81144265e-01   1.88557345e-02]\n",
      " [  7.74703842e-07   9.99999225e-01]\n",
      " [  9.93717305e-01   6.28269535e-03]\n",
      " [  9.99345680e-01   6.54320200e-04]\n",
      " [  1.00000000e+00   1.16757323e-10]]\n"
     ]
    }
   ],
   "source": [
    "import mdptoolbox_copy\n",
    "pi =  mdptoolbox_copy.mdp.PolicyIteration(P, R, discount=discount)\n",
    "pi.policy0=[1,1,1,1,1]\n",
    "#vi.setVerbose()\n",
    "pi.run()\n",
    "\n",
    "policy_pi = pi.policy\n",
    "\n",
    "print \"Optimal policy (policy iteration) : \\n\" , policy_pi\n",
    "\n",
    "policy_pg  = policy_gradient_algo( P, R , discount , path_len ,  n_paths, gamma=10 , eps=0.01)\n",
    "\n",
    "print \"Optimal policy (policy gradient) :\\n\" , mu_policy(policy_pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_rollout(mu_policy, transition_matrix, reward_matrix, discount, path_len=10) : \n",
    "    n_states, n_actions = mu_policy.shape\n",
    "    path_action = np.zeros(path_len , dtype=int)\n",
    "    path_state = np.zeros(path_len, dtype=int)\n",
    "    path_state[0] = rand.random_integers(n_states) -1\n",
    "    path_action[0] = np.random.choice(n_actions, 1, p=mu_policy[path_state[0]])\n",
    "    path_reward =  reward_matrix[path_state[0], path_action[0]]\n",
    "    for i in range(1, path_len) :\n",
    "        path_state[i] = np.random.choice(n_states , 1, p=transition_matrix[path_action[i-1]][path_state[i-1]])\n",
    "        path_action[i] = np.random.choice(n_actions, 1, p=mu_policy[path_state[i]])\n",
    "        path_reward =  path_reward + discount**i * reward_matrix[path_state[i], path_action[i]]\n",
    "    return path_state, path_action, path_reward\n",
    "\n",
    "\n",
    "def generate_rollouts(mu_policy, transition_matrix, reward_matrix, discount, path_len=10, n_paths=10):\n",
    "    n_states, n_actions = mu_policy.shape \n",
    "    paths_states = np.zeros(( n_paths , path_len), dtype=int)\n",
    "    paths_actions = np.zeros(( n_paths , path_len), dtype=int)\n",
    "    paths_rewards = np.zeros( n_paths  )\n",
    "    for i in range(0, n_paths) : \n",
    "        paths_states[i], paths_actions[i], paths_rewards[i] = generate_rollout(mu_policy, transition_matrix, reward_matrix, discount, path_len)\n",
    "    return paths_states, paths_actions, paths_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need function to generate rollouts from our policy. This will generate trajectories of our model under policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1 0]\n",
      " [3 4 4 0 0]\n",
      " [3 0 0 1 2]\n",
      " [3 4 0 1 0]\n",
      " [1 2 0 0 1]\n",
      " [3 0 0 0 0]\n",
      " [4 0 1 0 1]\n",
      " [3 0 0 1 0]\n",
      " [2 3 4 4 4]\n",
      " [4 0 1 0 1]] [[0 1 0 0 1]\n",
      " [0 0 1 1 0]\n",
      " [1 1 0 0 0]\n",
      " [0 1 0 1 1]\n",
      " [0 1 1 0 1]\n",
      " [1 1 1 1 0]\n",
      " [0 0 1 0 0]\n",
      " [1 1 0 1 1]\n",
      " [0 0 0 0 1]\n",
      " [1 0 1 0 1]] [ 0.      5.22    1.      2.529   1.5561  1.      4.81    1.729   7.4682\n",
      "  3.4661]\n"
     ]
    }
   ],
   "source": [
    "paths_states, paths_actions, paths_rewards = generate_rollouts(mu, P, R, discount, path_len=5, n_paths=10)\n",
    "print paths_states, paths_actions, paths_rewards"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
