{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy.random as rand\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def theta_policy(n_states, n_actions) : \n",
    "    theta = np.zeros((n_states, n_actions))    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\theta  - \\textrm{ parametrized policy}, \\theta \\in R^{ n \\times k }, \\textrm{where} $$\n",
    "$$ n=|S| - \\text{number of states} ,$$\n",
    "$$ k=|A| - \\text{number of actions} , $$\n",
    "$$ \\theta_{ij}   - \\textrm{ policy for state i  and action j }$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "theta = theta_policy(5, 2)\n",
    "print theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mu_policy( theta  ):\n",
    "    n_states, n_actions = theta.shape\n",
    "    mu  = np.zeros((n_states, n_actions))\n",
    "    for state in range(0, n_states) : \n",
    "        max_theta = np.max(theta[state])\n",
    "        mu[state] = np.exp(theta[state]-max_theta) / np.sum(np.exp(theta[state] - max_theta))\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mu - \\text{softmax policy} $$\n",
    "$$ $$ \n",
    "$$ \\mu(a=j | s=i; \\theta) = \\mu_{\\theta}(i,j) $$\n",
    "$$ $$\n",
    "$$  \\mu_{\\theta}(i,j)   =  \\frac{\\exp{\\theta_{ij}}}{\\sum_t{\\exp{\\theta_{it}}}} \\ $$\n",
    "\n",
    "$\\text{ computed using sum-exp trick} : $\n",
    "\n",
    "$$  \\frac{\\exp{\\theta_{ij}}}{\\sum_t{\\exp{\\theta_{it}}}} = \\frac{\\exp{\\theta_{ij}} \\times \\exp{(-a)}}{\\sum_t{\\exp{\\theta_{it}}} \\times \\exp{(-a)} } =  \\frac{\\exp{(\\theta_{ij} - a)}}{\\sum_t{\\exp{(\\theta_{it} - a)}}} $$\n",
    "\n",
    "$$ a = \\max_{t}{\\theta_{it}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5  0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]]\n"
     ]
    }
   ],
   "source": [
    "mu = mu_policy(theta)\n",
    "print mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_mu_policy( theta  ):\n",
    "    n_states, n_actions = theta.shape\n",
    "    log_mu  = np.zeros((n_states, n_actions))\n",
    "    for state in range(0, n_states) : \n",
    "        max_theta = np.max(theta[state])\n",
    "        log_sum_exp = max_theta  +  np.log(np.sum(np.exp(theta[state] - max_theta)))\n",
    "        log_mu[state] = theta[state] - log_sum_exp\n",
    "    return log_mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\log \\mu_{\\theta}(i,j)   = \\log \\frac{\\exp{\\theta_{ij}}}{\\sum_t{\\exp{\\theta_{it}}}} \\  = \\theta_{ij}  - \\log \\sum_t{\\exp{\\theta_{it}}}  = \\theta_{ij} - ( a + \\log \\sum_t{\\exp{(\\theta_{it}-a})}  )$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.69314718 -0.69314718]\n",
      " [-0.69314718 -0.69314718]\n",
      " [-0.69314718 -0.69314718]\n",
      " [-0.69314718 -0.69314718]\n",
      " [-0.69314718 -0.69314718]]\n"
     ]
    }
   ],
   "source": [
    "log_mu = log_mu_policy(theta)\n",
    "print log_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_log_mu_policy(theta, mu ,  state, action) :\n",
    "    n_states, n_actions = theta.shape\n",
    "    grad  = np.zeros((n_states, n_actions))\n",
    "    grad[state, action] = 1 \n",
    "    max_theta = np.max(theta[state])\n",
    "    grad[state] = grad[state] - mu[state]\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\nabla \\log \\mu_{\\theta}(i,j)  = \\nabla( \\theta_{ij}  - \\log \\sum_t{\\exp{\\theta_{it}}} )  = \\begin{pmatrix}\n",
    "  \\frac{\\partial \\log \\mu_{\\theta}(i,j)} {\\partial \\theta_{11}}  &  \\frac{\\partial \\log \\mu_{\\theta}(i,j)} {\\partial \\theta_{12}} & \\cdots &  \\frac{\\partial \\log \\mu_{\\theta}(i,j)} {\\partial \\theta_{1m}} \\\\\n",
    "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "  \\frac{\\partial \\log \\mu_{\\theta}(i,j)} {\\partial \\theta_{n1}} &  \\frac{\\partial \\log \\mu_{\\theta}(i,j)} {\\partial \\theta_{n2}} & \\cdots & \\frac{\\partial \\log \\mu_{\\theta}(i,j)} {\\partial \\theta_{nm}} \n",
    " \\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial ( \\log \\sum_t{\\exp{\\theta_{pt}}} ) } {\\partial \\theta_{pl}} =  \\frac{\\exp{\\theta_{pl}}}{\\sum_t{\\exp{\\theta_{pt}}} }  = \\mu_{\\theta}(p,l) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial \\log \\mu_{\\theta}(i,j)} {\\partial \\theta_{pl}} =  \\frac{\\partial ( \\theta_{ij}  - \\log \\sum_t{\\exp{\\theta_{it}}} ) } {\\partial \\theta_{pl}}  =  \\begin{cases}\n",
    "    1 -  \\mu_{\\theta}(p,l)      & \\quad \\text{if } i=p, j=l \\\\\n",
    "    -  \\mu_{\\theta}(p,l)     & \\quad \\text{if } i=p, j \\ne l \\\\\n",
    "    0                           & \\quad \\text{if }  i \\ne p, j \\ne l  \\\\\n",
    "  \\end{cases} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient log mu :\n",
      "[[ 0.   0. ]\n",
      " [ 0.5 -0.5]\n",
      " [ 0.   0. ]\n",
      " [ 0.   0. ]\n",
      " [ 0.   0. ]]\n",
      "mu policy after adding gradient :\n",
      "[[ 0.5         0.5       ]\n",
      " [ 0.26894142  0.73105858]\n",
      " [ 0.5         0.5       ]\n",
      " [ 0.5         0.5       ]\n",
      " [ 0.5         0.5       ]]\n"
     ]
    }
   ],
   "source": [
    "print \"gradient log mu :\\n\", gradient_log_mu_policy(theta, mu, 1, 0)\n",
    "print \"mu policy after adding gradient :\\n\" , mu_policy(theta + gradient_log_mu_policy(theta, mu, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def path_likelihood_ratio(theta_policy, mu_policy, path_states,  path_actions ) :\n",
    "    likelihood_ratio = np.zeros(theta_policy.shape)\n",
    "    path_len = len(path_states)\n",
    "    for t in range(0, path_len) :\n",
    "        likelihood_ratio = likelihood_ratio + gradient_log_mu_policy(theta_policy, mu_policy , path_states[t] , path_actions[t])\n",
    "    return likelihood_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likelihood ratio of path: \n",
    "$$ u(\\xi;\\theta)  =  \\sum_t{ \\nabla \\log \\mu_{\\theta}(s_t,a_t) } $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5  0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]]\n",
      "path state    :  [2 0 1 2 3]\n",
      "path  actions :  [1 0 0 0 1]\n",
      "path reward :  1.6561\n",
      "path likelihood ratio: \n",
      "[[ 0.5 -0.5]\n",
      " [ 0.5 -0.5]\n",
      " [ 0.   0. ]\n",
      " [-0.5  0.5]\n",
      " [ 0.   0. ]]\n",
      "path state    :  [3 0 1 0 0]\n",
      "path  actions :  [1 0 1 0 1]\n",
      "path reward :  1.81\n",
      "path likelihood ratio: \n",
      "[[ 0.5 -0.5]\n",
      " [-0.5  0.5]\n",
      " [ 0.   0. ]\n",
      " [-0.5  0.5]\n",
      " [ 0.   0. ]]\n",
      "path state    :  [2 3 4 4 4]\n",
      "path  actions :  [0 0 0 0 1]\n",
      "path reward :  7.4682\n",
      "path likelihood ratio: \n",
      "[[ 0.   0. ]\n",
      " [ 0.   0. ]\n",
      " [ 0.5 -0.5]\n",
      " [ 0.5 -0.5]\n",
      " [ 0.5 -0.5]]\n",
      "path state    :  [2 0 1 2 3]\n",
      "path  actions :  [0 0 0 0 0]\n",
      "path reward :  0.0\n",
      "path likelihood ratio: \n",
      "[[ 0.5 -0.5]\n",
      " [ 0.5 -0.5]\n",
      " [ 1.  -1. ]\n",
      " [ 0.5 -0.5]\n",
      " [ 0.   0. ]]\n",
      "path state    :  [0 0 1 2 0]\n",
      "path  actions :  [1 0 0 1 0]\n",
      "path reward :  0.729\n",
      "path likelihood ratio: \n",
      "[[ 0.5 -0.5]\n",
      " [ 0.5 -0.5]\n",
      " [-0.5  0.5]\n",
      " [ 0.   0. ]\n",
      " [ 0.   0. ]]\n",
      "path state    :  [2 0 1 0 1]\n",
      "path  actions :  [1 0 1 0 0]\n",
      "path reward :  1.81\n",
      "path likelihood ratio: \n",
      "[[ 1.  -1. ]\n",
      " [ 0.   0. ]\n",
      " [-0.5  0.5]\n",
      " [ 0.   0. ]\n",
      " [ 0.   0. ]]\n",
      "path state    :  [4 0 1 0 1]\n",
      "path  actions :  [1 0 1 0 1]\n",
      "path reward :  3.4661\n",
      "path likelihood ratio: \n",
      "[[ 1.  -1. ]\n",
      " [-1.   1. ]\n",
      " [ 0.   0. ]\n",
      " [ 0.   0. ]\n",
      " [-0.5  0.5]]\n",
      "path state    :  [3 0 0 1 0]\n",
      "path  actions :  [1 1 0 1 0]\n",
      "path reward :  1.729\n",
      "path likelihood ratio: \n",
      "[[ 0.5 -0.5]\n",
      " [-0.5  0.5]\n",
      " [ 0.   0. ]\n",
      " [-0.5  0.5]\n",
      " [ 0.   0. ]]\n",
      "path state    :  [1 2 3 0 1]\n",
      "path  actions :  [0 0 1 0 0]\n",
      "path reward :  0.81\n",
      "path likelihood ratio: \n",
      "[[ 0.5 -0.5]\n",
      " [ 1.  -1. ]\n",
      " [ 0.5 -0.5]\n",
      " [-0.5  0.5]\n",
      " [ 0.   0. ]]\n",
      "path state    :  [1 2 0 0 0]\n",
      "path  actions :  [0 1 1 1 1]\n",
      "path reward :  0.9\n",
      "path likelihood ratio: \n",
      "[[-1.5  1.5]\n",
      " [ 0.5 -0.5]\n",
      " [-0.5  0.5]\n",
      " [ 0.   0. ]\n",
      " [ 0.   0. ]]\n"
     ]
    }
   ],
   "source": [
    "print mu\n",
    "for i in range(0, paths_states.shape[0]) :\n",
    "    print \"path state    : \",paths_states[i]\n",
    "    print \"path  actions : \",paths_actions[i]\n",
    "    print \"path reward : \",paths_rewards[i]\n",
    "    print \"path likelihood ratio: \\n\", path_likelihood_ratio(theta, mu, paths_states[i], paths_actions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_gradient(theta_policy, mu_policy , paths_states , paths_actions, paths_rewards) :\n",
    "    policy_grad = np.zeros(theta_policy.shape)\n",
    "    n_paths , path_len =  paths_states.shape\n",
    "    for path in range(0, n_paths) :\n",
    "        policy_grad = policy_grad + paths_rewards[path]*path_likelihood_ratio(theta_policy, mu_policy, paths_states[path], paths_actions[path])\n",
    "    policy_grad = policy_grad/n_paths\n",
    "    return policy_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy gradient estimation :  \n",
    "$$ \\nabla \\eta(\\theta)  =  \\frac{1}{M} \\sum_m \\rho(\\xi_m) \\sum_t{ \\nabla \\log \\mu_{\\theta}(s_{mt},a_{mt}) } $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current policy mu : \n",
      "[[ 0.5  0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]]\n",
      "generated trajectories : \n",
      " states :\n",
      "[[2 0 1 2 3]\n",
      " [3 0 1 0 0]\n",
      " [2 3 4 4 4]\n",
      " [2 0 1 2 3]\n",
      " [0 0 1 2 0]\n",
      " [2 0 1 0 1]\n",
      " [4 0 1 0 1]\n",
      " [3 0 0 1 0]\n",
      " [1 2 3 0 1]\n",
      " [1 2 0 0 0]] \n",
      " actions :\n",
      "[[1 0 0 0 1]\n",
      " [1 0 1 0 1]\n",
      " [0 0 0 0 1]\n",
      " [0 0 0 0 0]\n",
      " [1 0 0 1 0]\n",
      " [1 0 1 0 0]\n",
      " [1 0 1 0 1]\n",
      " [1 1 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 1 1 1]] \n",
      " rewards :\n",
      "[ 1.6561  1.81    7.4682  0.      0.729   1.81    3.4661  1.729   0.81    0.9   ]\n",
      "policy gradient : \n",
      "[[ 0.729315 -0.729315]\n",
      " [-0.278305  0.278305]\n",
      " [ 0.24196  -0.24196 ]\n",
      " [ 0.073155 -0.073155]\n",
      " [ 0.200105 -0.200105]]\n",
      "new mu policy : \n",
      "[[ 0.81132305  0.18867695]\n",
      " [ 0.3643322   0.6356678 ]\n",
      " [ 0.6186731   0.3813269 ]\n",
      " [ 0.53651239  0.46348761]\n",
      " [ 0.59873811  0.40126189]]\n"
     ]
    }
   ],
   "source": [
    "print \"current policy mu : \\n\", mu_policy(theta)\n",
    "print \"generated trajectories : \\n states :\\n\" , paths_states, '\\n actions :\\n',  paths_actions, '\\n rewards :\\n', paths_rewards\n",
    "print \"policy gradient : \\n\",  policy_gradient(theta , mu, paths_states, paths_actions , paths_rewards)\n",
    "print \"new mu policy : \\n\",  mu_policy(theta + policy_gradient(theta , mu, paths_states, paths_actions , paths_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def policy_gradient_algo(transitions, rewards, discount, path_len=10,  n_paths=100, gamma=1.0, eps=0.01, n_iterations=100, logging=False) : \n",
    "    n_states, n_actions = rewards.shape\n",
    "    theta = theta_policy(n_states, n_actions)\n",
    "    mu = mu_policy(theta)\n",
    "    n=0\n",
    "    paths_states, paths_actions, paths_rewards = generate_rollouts(mu, transitions, rewards, discount , path_len, n_paths )\n",
    "    pgrad = policy_gradient(theta, mu,  paths_states, paths_actions, paths_rewards) \n",
    "    theta_diff =  (gamma/(n+1))*pgrad \n",
    "    theta_diff_norm = np.linalg.norm(theta_diff)\n",
    "    #mu_diff = np.linalg.norm(mu_policy(theta) - mu_policy(theta+pgrad))\n",
    "    while ( (n<n_iterations) & (theta_diff_norm>eps) ):\n",
    "        if (logging) :\n",
    "            print \"mu policy : \\n\",  mu_policy(theta)\n",
    "            print \"policy gradient: \\n\", pgrad\n",
    "            print \"theta policy : \\n\" , theta_diff\n",
    "            print \"theta policy diff norm: \" , theta_diff_norm\n",
    "        theta_diff =  (gamma/(n+1))*pgrad\n",
    "        theta = theta + theta_diff\n",
    "        mu = mu_policy(theta)\n",
    "        paths_states, paths_actions, paths_rewards = generate_rollouts(mu_policy(theta), transitions, rewards, discount , path_len, n_paths )\n",
    "        pgrad = policy_gradient(theta, mu , paths_states, paths_actions, paths_rewards)\n",
    "        #mu_diff = np.linalg.norm(mu_policy(theta) - mu_policy(theta+pgrad))\n",
    "        theta_diff_norm = np.linalg.norm(theta_diff) \n",
    "        n = n+1\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "repeat\n",
    "$$ \\theta^{n+1} = \\theta^{n} + \\frac {\\gamma}{n}  * \\nabla \\eta(\\theta^n) $$\n",
    "until \n",
    "$$ || \\theta_{n+1} - \\theta_{n} || > \\epsilon$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.34621791, -5.34621791],\n",
       "       [-5.74054404,  5.74054404],\n",
       "       [ 2.59020224, -2.59020224],\n",
       "       [ 2.99125497, -2.99125497],\n",
       "       [ 4.46383365, -4.46383365]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_gradient_algo(P, R , discount,  path_len=10, gamma = 10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare with other existing method  - policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy (policy iteration) : \n",
      "(0, 0, 0, 0, 0)\n",
      "Optimal policy (policy gradient) :\n",
      "[[  9.84749183e-01   1.52508172e-02]\n",
      " [  1.54634257e-03   9.98453657e-01]\n",
      " [  7.96454703e-02   9.20354530e-01]\n",
      " [  9.98535462e-01   1.46453781e-03]\n",
      " [  1.00000000e+00   7.57949729e-13]]\n"
     ]
    }
   ],
   "source": [
    "import mdptoolbox_copy\n",
    "pi =  mdptoolbox_copy.mdp.PolicyIteration(P, R, discount=discount)\n",
    "pi.policy0=[1,1,1,1,1]\n",
    "#vi.setVerbose()\n",
    "pi.run()\n",
    "\n",
    "policy_pi = pi.policy\n",
    "\n",
    "print \"Optimal policy (policy iteration) : \\n\" , policy_pi\n",
    "\n",
    "policy_pg  = policy_gradient_algo( P, R , discount , path_len ,  n_paths, gamma=10 , eps=0.01)\n",
    "\n",
    "print \"Optimal policy (policy gradient) :\\n\" , mu_policy(policy_pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mdptoolbox_copy.example as mdp_ex\n",
    "\n",
    "n_states = 5\n",
    "n_actions = 2\n",
    "fire_prob = 0.1\n",
    "discount=0.9\n",
    "n_paths=100\n",
    "path_len=100\n",
    "path_len = 10\n",
    "path_num = 10\n",
    "\n",
    "P, R = mdp_ex.forest(S=n_states, p=fire_prob)\n",
    "\n",
    "def generate_rollout(mu_policy, transition_matrix, reward_matrix, discount, path_len=10) : \n",
    "    n_states, n_actions = mu_policy.shape\n",
    "    path_action = np.zeros(path_len , dtype=int)\n",
    "    path_state = np.zeros(path_len, dtype=int)\n",
    "    path_state[0] = rand.random_integers(n_states) -1\n",
    "    path_action[0] = np.random.choice(n_actions, 1, p=mu_policy[path_state[0]])\n",
    "    path_reward =  reward_matrix[path_state[0], path_action[0]]\n",
    "    for i in range(1, path_len) :\n",
    "        path_state[i] = np.random.choice(n_states , 1, p=transition_matrix[path_action[i-1]][path_state[i-1]])\n",
    "        path_action[i] = np.random.choice(n_actions, 1, p=mu_policy[path_state[i]])\n",
    "        path_reward =  path_reward + discount**i * reward_matrix[path_state[i], path_action[i]]\n",
    "    return path_state, path_action, path_reward\n",
    "\n",
    "\n",
    "def generate_rollouts(mu_policy, transition_matrix, reward_matrix, discount, path_len=10, n_paths=10):\n",
    "    n_states, n_actions = mu_policy.shape \n",
    "    paths_states = np.zeros(( n_paths , path_len), dtype=int)\n",
    "    paths_actions = np.zeros(( n_paths , path_len), dtype=int)\n",
    "    paths_rewards = np.zeros( n_paths  )\n",
    "    for i in range(0, n_paths) : \n",
    "        paths_states[i], paths_actions[i], paths_rewards[i] = generate_rollout(mu_policy, transition_matrix, reward_matrix, discount, path_len)\n",
    "    return paths_states, paths_actions, paths_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need function to generate rollouts from our policy. This will generate trajectories of our model under policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1 0]\n",
      " [3 4 4 0 0]\n",
      " [3 0 0 1 2]\n",
      " [3 4 0 1 0]\n",
      " [1 2 0 0 1]\n",
      " [3 0 0 0 0]\n",
      " [4 0 1 0 1]\n",
      " [3 0 0 1 0]\n",
      " [2 3 4 4 4]\n",
      " [4 0 1 0 1]] [[0 1 0 0 1]\n",
      " [0 0 1 1 0]\n",
      " [1 1 0 0 0]\n",
      " [0 1 0 1 1]\n",
      " [0 1 1 0 1]\n",
      " [1 1 1 1 0]\n",
      " [0 0 1 0 0]\n",
      " [1 1 0 1 1]\n",
      " [0 0 0 0 1]\n",
      " [1 0 1 0 1]] [ 0.      5.22    1.      2.529   1.5561  1.      4.81    1.729   7.4682\n",
      "  3.4661]\n"
     ]
    }
   ],
   "source": [
    "paths_states, paths_actions, paths_rewards = generate_rollouts(mu, P, R, discount, path_len=5, n_paths=10)\n",
    "print paths_states, paths_actions, paths_rewards"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
